/**
 * Historical Validation Module
 * Validates prediction models against real match results for evidence-based accuracy assessment
 * 
 * ACADEMIC REFERENCES:
 * - Constantinou, A.C. & Fenton, N.E. (2012). "Solving the problem of inadequate scoring rules for assessing probabilistic football forecast models"
 *   DOI: 10.1080/02664763.2013.784894
 * - Hvattum, L.M. & Arntzen, H. (2010). "Using ELO ratings for match result prediction in association football"
 *   DOI: 10.1515/ijsm.2010.28.6.563
 * - Koopman, S.J. & Lit, R. (2015). "A dynamic bivariate Poisson model for analysing and forecasting match results in the English Premier League"
 *   DOI: 10.1111/rssa.12042
 */

import { calculateAdvancedXGPrediction } from './xgCalculations.js';
import { EnhancedPoissonCalculator } from './improvedMath.js';
import { MonteCarloSimulator } from './improvedMath.js';

export class HistoricalValidator {
  constructor() {
    this.poissonCalculator = new EnhancedPoissonCalculator();
    this.monteCarloSimulator = new MonteCarloSimulator();
    this.validationResults = new Map();
  }

  /**
   * Validate model predictions against historical match results
   * @param {Array} historicalMatches - Array of historical match data
   * @param {string} modelType - Type of model to validate ('xg', 'poisson', 'montecarlo')
   * @returns {Object} Validation metrics and accuracy scores
   */
  async validateModel(historicalMatches, modelType = 'xg') {
    console.log(`ðŸ” Validating ${modelType} model against ${historicalMatches.length} historical matches...`);
    
    const predictions = [];
    const actualResults = [];
    const calibrationData = { bins: 10, predicted: [], actual: [] };
    
    let correctPredictions = 0;
    let totalLogLikelihood = 0;
    let brierScore = 0;
    
    for (const match of historicalMatches) {
      try {
        // Generate prediction based on model type
        const prediction = await this.generatePrediction(match, modelType);
        const actual = this.extractActualResult(match);
        
        predictions.push(prediction);
        actualResults.push(actual);
        
        // Calculate accuracy metrics
        const isCorrect = this.isPredictionCorrect(prediction, actual);
        if (isCorrect) correctPredictions++;
        
        // Calculate Brier Score (lower is better)
        const brierContribution = this.calculateBrierScore(prediction, actual);
        brierScore += brierContribution;
        
        // Calculate Log Likelihood
        const logLikelihood = this.calculateLogLikelihood(prediction, actual);
        totalLogLikelihood += logLikelihood;
        
        // Collect calibration data
        this.collectCalibrationData(prediction, actual, calibrationData);
        
      } catch (error) {
        console.warn(`âš ï¸ Skipping match due to error: ${error.message}`);
      }
    }
    
    const validationMetrics = {
      modelType,
      totalMatches: historicalMatches.length,
      validPredictions: predictions.length,
      accuracy: (correctPredictions / predictions.length) * 100,
      brierScore: brierScore / predictions.length,
      logLikelihood: totalLogLikelihood / predictions.length,
      calibration: this.calculateCalibration(calibrationData),
      sharpness: this.calculateSharpness(predictions),
      reliability: this.calculateReliability(calibrationData)
    };
    
    // Store results for comparison
    this.validationResults.set(modelType, validationMetrics);
    
    console.log(`âœ… ${modelType} validation complete:`);
    console.log(`   Accuracy: ${validationMetrics.accuracy.toFixed(2)}%`);
    console.log(`   Brier Score: ${validationMetrics.brierScore.toFixed(4)}`);
    console.log(`   Log Likelihood: ${validationMetrics.logLikelihood.toFixed(4)}`);
    
    return validationMetrics;
  }
  
  /**
   * Generate prediction using specified model
   * @param {Object} match - Match data with team stats
   * @param {string} modelType - Model type to use
   * @returns {Object} Prediction probabilities
   */
  async generatePrediction(match, modelType) {
    const params = this.extractMatchParameters(match);
    
    switch (modelType) {
      case 'xg':
        return calculateAdvancedXGPrediction(params.xgParams);
      
      case 'poisson':
        return this.poissonCalculator.calculate(params.poissonParams);
      
      case 'montecarlo':
        return this.monteCarloSimulator.simulate(params.monteCarloParams);
      
      default:
        throw new Error(`Unknown model type: ${modelType}`);
    }
  }
  
  /**
   * Extract match parameters from historical data
   * @param {Object} match - Historical match data
   * @returns {Object} Parameters for different models
   */
  extractMatchParameters(match) {
    // Extract or estimate parameters from historical match data
    // This would typically come from pre-match statistics
    return {
      xgParams: {
        homeXG: match.homeXG || this.estimateXG(match.homeTeam, match.season),
        awayXG: match.awayXG || this.estimateXG(match.awayTeam, match.season),
        homeAdvantage: match.homeAdvantage || 0.3,
        recentForm: match.recentForm || 0.5,
        homeDefense: match.homeDefense || 1.0,
        awayDefense: match.awayDefense || 1.0,
        motivation: match.motivation || 0.5
      },
      poissonParams: {
        homeAttackRate: match.homeAttackRate || 1.5,
        awayAttackRate: match.awayAttackRate || 1.3,
        homeDefenseRate: match.homeDefenseRate || 1.0,
        awayDefenseRate: match.awayDefenseRate || 1.1
      },
      monteCarloParams: {
        homeGoalsAvg: match.homeGoalsAvg || 1.5,
        awayGoalsAvg: match.awayGoalsAvg || 1.2,
        homeAdvantage: match.homeAdvantage || 0.3,
        simulations: 5000
      }
    };
  }
  
  /**
   * Estimate xG for a team based on historical performance
   * @param {string} team - Team name
   * @param {string} season - Season identifier
   * @returns {number} Estimated xG value
   */
  estimateXG(team, season) {
    // Simplified estimation - in practice, this would use comprehensive historical data
    const baseXG = 1.4; // League average
    const teamModifier = Math.random() * 0.6 - 0.3; // Â±0.3 variation
    return Math.max(0.5, baseXG + teamModifier);
  }
  
  /**
   * Extract actual match result
   * @param {Object} match - Match data with actual result
   * @returns {Object} Actual result (homeGoals, awayGoals, result)
   */
  extractActualResult(match) {
    return {
      homeGoals: match.homeGoals,
      awayGoals: match.awayGoals,
      result: match.homeGoals > match.awayGoals ? 'home' : 
              match.homeGoals < match.awayGoals ? 'away' : 'draw'
    };
  }
  
  /**
   * Check if prediction correctly identified the match outcome
   * @param {Object} prediction - Model prediction
   * @param {Object} actual - Actual result
   * @returns {boolean} True if prediction was correct
   */
  isPredictionCorrect(prediction, actual) {
    const homeWin = parseFloat(prediction.homeWinProbability);
    const draw = parseFloat(prediction.drawProbability);
    const awayWin = parseFloat(prediction.awayWinProbability);
    
    const predictedResult = homeWin > draw && homeWin > awayWin ? 'home' :
                           awayWin > draw && awayWin > homeWin ? 'away' : 'draw';
    
    return predictedResult === actual.result;
  }
  
  /**
   * Calculate Brier Score for probabilistic predictions
   * @param {Object} prediction - Model prediction
   * @param {Object} actual - Actual result
   * @returns {number} Brier score contribution
   */
  calculateBrierScore(prediction, actual) {
    const homeProb = parseFloat(prediction.homeWinProbability) / 100;
    const drawProb = parseFloat(prediction.drawProbability) / 100;
    const awayProb = parseFloat(prediction.awayWinProbability) / 100;
    
    const homeActual = actual.result === 'home' ? 1 : 0;
    const drawActual = actual.result === 'draw' ? 1 : 0;
    const awayActual = actual.result === 'away' ? 1 : 0;
    
    return Math.pow(homeProb - homeActual, 2) + 
           Math.pow(drawProb - drawActual, 2) + 
           Math.pow(awayProb - awayActual, 2);
  }
  
  /**
   * Calculate Log Likelihood for probabilistic predictions
   * @param {Object} prediction - Model prediction
   * @param {Object} actual - Actual result
   * @returns {number} Log likelihood contribution
   */
  calculateLogLikelihood(prediction, actual) {
    const homeProb = parseFloat(prediction.homeWinProbability) / 100;
    const drawProb = parseFloat(prediction.drawProbability) / 100;
    const awayProb = parseFloat(prediction.awayWinProbability) / 100;
    
    let actualProb;
    switch (actual.result) {
      case 'home': actualProb = homeProb; break;
      case 'draw': actualProb = drawProb; break;
      case 'away': actualProb = awayProb; break;
      default: actualProb = 0.001; // Avoid log(0)
    }
    
    return Math.log(Math.max(actualProb, 0.001)); // Avoid log(0)
  }
  
  /**
   * Collect data for calibration analysis
   * @param {Object} prediction - Model prediction
   * @param {Object} actual - Actual result
   * @param {Object} calibrationData - Data collection object
   */
  collectCalibrationData(prediction, actual, calibrationData) {
    const homeProb = parseFloat(prediction.homeWinProbability) / 100;
    const drawProb = parseFloat(prediction.drawProbability) / 100;
    const awayProb = parseFloat(prediction.awayWinProbability) / 100;
    
    // Collect the highest probability prediction
    const maxProb = Math.max(homeProb, drawProb, awayProb);
    const predictedOutcome = homeProb === maxProb ? 'home' :
                            drawProb === maxProb ? 'draw' : 'away';
    
    calibrationData.predicted.push(maxProb);
    calibrationData.actual.push(predictedOutcome === actual.result ? 1 : 0);
  }
  
  /**
   * Calculate calibration metrics
   * @param {Object} calibrationData - Collected calibration data
   * @returns {Object} Calibration analysis
   */
  calculateCalibration(calibrationData) {
    const bins = calibrationData.bins;
    const binSize = 1.0 / bins;
    const binResults = Array(bins).fill(0).map(() => ({ predicted: [], actual: [] }));
    
    // Sort data into bins
    for (let i = 0; i < calibrationData.predicted.length; i++) {
      const prob = calibrationData.predicted[i];
      const binIndex = Math.min(Math.floor(prob / binSize), bins - 1);
      binResults[binIndex].predicted.push(prob);
      binResults[binIndex].actual.push(calibrationData.actual[i]);
    }
    
    // Calculate calibration for each bin
    const calibrationScore = binResults.map((bin, index) => {
      if (bin.predicted.length === 0) return null;
      
      const avgPredicted = bin.predicted.reduce((a, b) => a + b, 0) / bin.predicted.length;
      const avgActual = bin.actual.reduce((a, b) => a + b, 0) / bin.actual.length;
      
      return {
        binIndex: index,
        binRange: [index * binSize, (index + 1) * binSize],
        count: bin.predicted.length,
        avgPredicted: avgPredicted,
        avgActual: avgActual,
        calibrationError: Math.abs(avgPredicted - avgActual)
      };
    }).filter(bin => bin !== null);
    
    const overallCalibrationError = calibrationScore.reduce((sum, bin) => 
      sum + bin.calibrationError * bin.count, 0) / calibrationData.predicted.length;
    
    return {
      bins: calibrationScore,
      overallCalibrationError,
      isWellCalibrated: overallCalibrationError < 0.1
    };
  }
  
  /**
   * Calculate sharpness (how confident the model is)
   * @param {Array} predictions - Array of predictions
   * @returns {number} Sharpness score
   */
  calculateSharpness(predictions) {
    let totalEntropy = 0;
    
    for (const prediction of predictions) {
      const homeProb = parseFloat(prediction.homeWinProbability) / 100;
      const drawProb = parseFloat(prediction.drawProbability) / 100;
      const awayProb = parseFloat(prediction.awayWinProbability) / 100;
      
      // Calculate entropy (lower entropy = higher sharpness)
      const entropy = -(
        homeProb * Math.log2(Math.max(homeProb, 0.001)) +
        drawProb * Math.log2(Math.max(drawProb, 0.001)) +
        awayProb * Math.log2(Math.max(awayProb, 0.001))
      );
      
      totalEntropy += entropy;
    }
    
    const avgEntropy = totalEntropy / predictions.length;
    const maxEntropy = Math.log2(3); // Maximum entropy for 3 outcomes
    
    return 1 - (avgEntropy / maxEntropy); // Convert to sharpness (higher is better)
  }
  
  /**
   * Calculate reliability from calibration data
   * @param {Object} calibrationData - Calibration data
   * @returns {number} Reliability score
   */
  calculateReliability(calibrationData) {
    // Reliability is the weighted average of squared differences between
    // predicted probabilities and actual frequencies
    const calibration = this.calculateCalibration(calibrationData);
    return 1 - calibration.overallCalibrationError; // Convert to positive score
  }
  
  /**
   * Generate comprehensive validation report
   * @returns {Object} Complete validation report
   */
  generateValidationReport() {
    const report = {
      timestamp: new Date().toISOString(),
      models: {},
      comparison: {},
      recommendations: []
    };
    
    // Add individual model results
    for (const [modelType, metrics] of this.validationResults) {
      report.models[modelType] = metrics;
    }
    
    // Compare models if multiple exist
    if (this.validationResults.size > 1) {
      report.comparison = this.compareModels();
    }
    
    // Generate recommendations
    report.recommendations = this.generateRecommendations();
    
    return report;
  }
  
  /**
   * Compare different models
   * @returns {Object} Model comparison results
   */
  compareModels() {
    const models = Array.from(this.validationResults.entries());
    
    const bestAccuracy = models.reduce((best, [name, metrics]) => 
      metrics.accuracy > best.accuracy ? { name, accuracy: metrics.accuracy } : best,
      { name: '', accuracy: 0 }
    );
    
    const bestBrierScore = models.reduce((best, [name, metrics]) => 
      metrics.brierScore < best.brierScore ? { name, brierScore: metrics.brierScore } : best,
      { name: '', brierScore: Infinity }
    );
    
    const bestCalibration = models.reduce((best, [name, metrics]) => 
      metrics.reliability > best.reliability ? { name, reliability: metrics.reliability } : best,
      { name: '', reliability: 0 }
    );
    
    return {
      bestAccuracy,
      bestBrierScore,
      bestCalibration,
      overallRecommendation: this.determineOverallBest(models)
    };
  }
  
  /**
   * Determine overall best model
   * @param {Array} models - Array of model results
   * @returns {string} Best model name
   */
  determineOverallBest(models) {
    // Weighted scoring: accuracy (40%), Brier score (30%), reliability (30%)
    let bestScore = -Infinity;
    let bestModel = '';
    
    for (const [name, metrics] of models) {
      const score = (metrics.accuracy * 0.4) + 
                   ((1 - metrics.brierScore) * 30) + // Lower Brier is better
                   (metrics.reliability * 30);
      
      if (score > bestScore) {
        bestScore = score;
        bestModel = name;
      }
    }
    
    return bestModel;
  }
  
  /**
   * Generate recommendations based on validation results
   * @returns {Array} Array of recommendation strings
   */
  generateRecommendations() {
    const recommendations = [];
    
    for (const [modelType, metrics] of this.validationResults) {
      if (metrics.accuracy < 40) {
        recommendations.push(`${modelType} model has low accuracy (${metrics.accuracy.toFixed(1)}%) - consider parameter tuning`);
      }
      
      if (metrics.brierScore > 0.3) {
        recommendations.push(`${modelType} model has poor probability calibration - review probability calculations`);
      }
      
      if (metrics.reliability < 0.7) {
        recommendations.push(`${modelType} model shows poor reliability - predictions may be overconfident`);
      }
      
      if (metrics.sharpness < 0.3) {
        recommendations.push(`${modelType} model lacks sharpness - predictions are too conservative`);
      }
    }
    
    if (recommendations.length === 0) {
      recommendations.push('All models show good performance metrics');
    }
    
    return recommendations;
  }
  
  /**
   * Clear validation results
   */
  clearResults() {
    this.validationResults.clear();
  }
}

// Export validation utilities
export const createSampleHistoricalData = () => {
  // Generate sample historical match data for testing
  const sampleMatches = [];
  
  for (let i = 0; i < 100; i++) {
    const homeGoals = Math.floor(Math.random() * 5);
    const awayGoals = Math.floor(Math.random() * 4);
    
    sampleMatches.push({
      id: i + 1,
      homeTeam: `Team ${String.fromCharCode(65 + (i % 10))}`,
      awayTeam: `Team ${String.fromCharCode(75 + (i % 10))}`,
      homeGoals,
      awayGoals,
      homeXG: 1.2 + Math.random() * 1.6,
      awayXG: 1.0 + Math.random() * 1.4,
      homeAdvantage: 0.2 + Math.random() * 0.2,
      season: '2023-24',
      date: new Date(2023, Math.floor(Math.random() * 12), Math.floor(Math.random() * 28) + 1)
    });
  }
  
  return sampleMatches;
};

export default HistoricalValidator;